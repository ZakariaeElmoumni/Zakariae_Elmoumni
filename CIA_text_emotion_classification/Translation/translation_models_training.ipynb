{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213bf3b6-2ad1-44ec-8a46-bbe21875365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tudor\\anaconda3\\envs\\y2c\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "from transformers import MBart50Tokenizer, TFTrainingArguments, TFMBartForConditionalGeneration, DataCollatorForSeq2Seq, create_optimizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0aff23c-c3fc-45a9-bef2-67cde038b579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29aaaefd46584e869a0a24b26479d288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "kde4.py:   0%|          | 0.00/4.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7245183404b4b07996f9669a6e648fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d23e2e96fd4119bf60d90411f2f5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac2ec8a9ccf46a7b005152ce26a3e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/192060 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "{'en': 'Lauri Watts', 'nl': 'Lauri Watts'}\n",
      "\n",
      "Example 1:\n",
      "{'en': '& Lauri. Watts. mail;', 'nl': '& Lauri.Watts.mail;'}\n",
      "\n",
      "Example 2:\n",
      "{'en': 'ROLES_OF_TRANSLATORS', 'nl': '& Niels.Reedijk; Bram.Schoenmakers; Natalie.Koning;'}\n",
      "\n",
      "Example 3:\n",
      "{'en': '2006-02-26 3.5.1', 'nl': '2006-02-26 3.5.1'}\n",
      "\n",
      "Example 4:\n",
      "{'en': 'The Babel & konqueror; plugin gives you quick access to the Babelfish translation service.', 'nl': 'De plugin Babel voor & konqueror; geeft u snel en gemakkelijk toegang tot de Babelfish-vertaaldienst.'}\n",
      "\n",
      "Example 5:\n",
      "{'en': 'KDE', 'nl': 'KDE'}\n",
      "\n",
      "Example 6:\n",
      "{'en': 'kdeaddons', 'nl': 'kdeaddons'}\n",
      "\n",
      "Example 7:\n",
      "{'en': 'konqueror', 'nl': 'konqueror'}\n",
      "\n",
      "Example 8:\n",
      "{'en': 'plugins', 'nl': 'plugins'}\n",
      "\n",
      "Example 9:\n",
      "{'en': 'babelfish', 'nl': 'babelfish'}\n",
      "\n",
      "Example 10:\n",
      "{'en': 'translate', 'nl': 'translate'}\n",
      "\n",
      "Example 11:\n",
      "{'en': 'The Babel & konqueror; plugin', 'nl': 'De & konqueror; Babel-plugin'}\n",
      "\n",
      "Example 12:\n",
      "{'en': 'Using the Babelfish plugin', 'nl': 'De Babelfish-plugin gebruiken'}\n",
      "\n",
      "Example 13:\n",
      "{'en': 'Babelfish is a machine translation service provided by AltaVista.', 'nl': 'Babelf ish is een vertaaldienst die geleverd wordt door AltaVista.'}\n",
      "\n",
      "Example 14:\n",
      "{'en': 'The plugin allows you to automatically translate web pages between several languages.', 'nl': \"Deze plugin maakt het mogelijk om automatisch webpagina's te vertalen naar verschillende talen.\"}\n",
      "\n",
      "Example 15:\n",
      "{'en': 'The Babelfish plugin can be accessed in the & konqueror; menubar under Tools Translate Web Page. Select from the list that drops down the language to translate from and the language to translate to.', 'nl': 'U kunt de Babelfish-plugin vinden in het & konqueror; -menu bij Hulpmiddelen Webpagina vertalen. Kies in de uitvouwlijst de taal van waaruit u wilt vertalen en de taal waarin u wilt vertalen.'}\n",
      "\n",
      "Example 16:\n",
      "{'en': 'If you have a portion of the text on the page selected, that will be translated instead of the entire & URL;.', 'nl': 'Wanneer u een tekstgedeelte geselecteerd hebt, zal alleen dat gedeelte vertaald worden in plaats van de hele pagina.'}\n",
      "\n",
      "Example 17:\n",
      "{'en': 'Not all languages are available from the Babelfish service.', 'nl': 'Niet alle talen zijn beschikbaar via de dienst Babelfish.'}\n",
      "\n",
      "Example 18:\n",
      "{'en': \"Machine translation is not a perfect science! Babelfish will at best give you a rough translation, and at worst will give you a very funny read. Do n't base important decisions on things you have read from a Babelfish translated page, without confirming that the translation is indeed accurate.\", 'nl': 'Vertaling door een machine is nooit foutloos! Babelfish zal hoogstens een ruwe vertaling geven. Wanneer u minder geluk hebt zal er een lachwekkende vertaling uitkomen. Baseer belangrijke informatie niet op een pagina die door Babelfish is vertaald zonder dat u werkelijk nagaat of de vertaling correct was.'}\n",
      "\n",
      "Example 19:\n",
      "{'en': 'You may only translate web pages that are accessible on the world wide web. To translate any other text, you should go directly to the Babelfish site itself, where you are able to paste in text for translation.', 'nl': \"Het is alleen mogelijk om webpagina's te vertalen die op het World Wide Web staan. Om andere teksten te vertalen dient u naar de site van Babelfish zelf te gaan, daar kunt u de tekst plakken die u wilt vertalen.\"}\n",
      "\n",
      "Example 20:\n",
      "{'en': 'Credits', 'nl': 'Dankbetuigingen'}\n",
      "\n",
      "Example 21:\n",
      "{'en': 'The Babelfish plugin is copyright (C) 2001 & Kurt. Granroth; & Kurt. Granroth. mail;', 'nl': 'De Babelfish-plugin heeft copyright (C) 2001 & Kurt.Granroth; & Kurt.Granroth.mail;'}\n",
      "\n",
      "Example 22:\n",
      "{'en': 'Documentation Copyright 2002 & Lauri. Watts; & Lauri. Watts. mail;', 'nl': 'Documentatie Copyright 2002 & Lauri.Watts; & Lauri.Watts.mail;'}\n",
      "\n",
      "Example 23:\n",
      "{'en': 'CREDIT_FOR_TRANSLATORS', 'nl': '& meld.fouten; vertaling.niels; vertaling.bram; nagelezen.rinse; nagelezen.natalie;'}\n",
      "\n",
      "Example 24:\n",
      "{'en': 'The & konqueror; Plugins Handbook', 'nl': 'Het handboek van de & konqueror; -plugins'}\n",
      "\n",
      "Example 25:\n",
      "{'en': 'Lauri Watts', 'nl': 'Lauri Watts'}\n",
      "\n",
      "Example 26:\n",
      "{'en': '& Lauri. Watts. mail;', 'nl': '& Lauri.Watts.mail;'}\n",
      "\n",
      "Example 27:\n",
      "{'en': 'ROLES_OF_TRANSLATORS', 'nl': '& Niels.Reedijk; Bram.Schoenmakers; Natalie.Koning;'}\n",
      "\n",
      "Example 28:\n",
      "{'en': '& Lauri. Watts;', 'nl': '& Lauri.Watts;'}\n",
      "\n",
      "Example 29:\n",
      "{'en': 'This is the handbook for the additional plugins for & konqueror; the & kde; file manager and web browser.', 'nl': 'Dit handboek gaat over de extra plugins die u kunt toevoegen aan & konqueror;, de bestandsbeheerder en webbrowser van & kde;.'}\n",
      "\n",
      "Example 30:\n",
      "{'en': 'KDE', 'nl': 'KDE'}\n",
      "\n",
      "Example 31:\n",
      "{'en': 'kdeaddons', 'nl': 'kdeaddons'}\n",
      "\n",
      "Example 32:\n",
      "{'en': 'Konqueror', 'nl': 'Konqueror'}\n",
      "\n",
      "Example 33:\n",
      "{'en': 'Plugins', 'nl': 'Plugins'}\n",
      "\n",
      "Example 34:\n",
      "{'en': 'Introduction', 'nl': 'Inleiding'}\n",
      "\n",
      "Example 35:\n",
      "{'en': 'The & package; package contains several plugins for & konqueror;. Despite their small size, they provide additional functions and ease the use of some of the functions & konqueror; already contains, by making them more accessible.', 'nl': 'Het pakket & package; bevat enkele plugins voor & konqueror;. Ondanks dat ze zo klein zijn brengen ze toch een heleboel functionaliteit met zich mee en maken ze sommige functies beter bereikbaar.'}\n",
      "\n",
      "Example 36:\n",
      "{'en': 'More information on the use of & konqueror; and its built in functionality, can be found in the & konqueror; handbook.', 'nl': 'Meer informatie over het gebruik van & konqueror; en de mogelijkheden ervan kunt u in het handboek van & konqueror; vinden.'}\n",
      "\n",
      "Example 37:\n",
      "{'en': 'To load these plugins from within & konqueror;, select Settings Configure Extensions.... Go to the Tools tab and choose the wanted plugins.', 'nl': 'U kunt deze plugins laden terwijl & konqueror; geopend is, kies Instellingen Extensies instellen... in het menu. Klik op het tabblad Hulpmiddelen en kies de gewenste plugins.'}\n",
      "\n",
      "Example 38:\n",
      "{'en': 'This is a list of the standard configuration modules provided by the kde; addons package. Please note that there may be many more modules on your system if you have installed additional software.', 'nl': 'Hier volgt een lijst van standaard plugins die zich in het pakket kdeaddons bevinden. Het kan zijn dat er veel meer plugins op uw computer staan wanneer u extra software hebt geÃ­nstalleerd.'}\n",
      "\n",
      "Example 39:\n",
      "{'en': 'Babelfish', 'nl': 'Babelfish'}\n",
      "\n",
      "Example 40:\n",
      "{'en': 'The DOM tree viewer', 'nl': 'De DOM -boomstructuurweergave'}\n",
      "\n",
      "Example 41:\n",
      "{'en': 'The & konqueror; image gallery', 'nl': 'De & konqueror; afbeeldingengalerij'}\n",
      "\n",
      "Example 42:\n",
      "{'en': 'The Embedded Media Player', 'nl': 'De ingebouwde Mediaspeler'}\n",
      "\n",
      "Example 43:\n",
      "{'en': 'The Validators', 'nl': 'De validators'}\n",
      "\n",
      "Example 44:\n",
      "{'en': 'Kuick', 'nl': 'Kuick'}\n",
      "\n",
      "Example 45:\n",
      "{'en': 'The Folder Filter', 'nl': 'De mappenfilter'}\n",
      "\n",
      "Example 46:\n",
      "{'en': 'KHTMLSettings', 'nl': 'KHTML-instellingen'}\n",
      "\n",
      "Example 47:\n",
      "{'en': 'The User Agent Changer', 'nl': 'De browseridentificatie wijzigen'}\n",
      "\n",
      "Example 48:\n",
      "{'en': 'The Web Archiver', 'nl': 'De webarchiveerder'}\n",
      "\n",
      "Example 49:\n",
      "{'en': 'The Crashes Plugin', 'nl': 'De Crashes-plugin'}\n",
      "\n",
      "Example 50:\n",
      "{'en': 'FSView - the File System View', 'nl': 'FSView - de bestandssysteemweergave'}\n",
      "\n",
      "Example 51:\n",
      "{'en': 'Credits and License', 'nl': 'Dankbetuigingen en licentie'}\n",
      "\n",
      "Example 52:\n",
      "{'en': 'Copyrights for each plugin are listed in the applicable chapter.', 'nl': 'Copyrights voor iedere plugin staan vermeld in het bijbehorende hoofdstuk.'}\n",
      "\n",
      "Example 53:\n",
      "{'en': 'CREDIT_FOR_TRANSLATORS', 'nl': '& meld.fouten; vertaling.niels; vertaling.bram; nagelezen.natalie;'}\n",
      "\n",
      "Example 54:\n",
      "{'en': '& underFDL; & underGPL;', 'nl': '& underFDL; & underGPL;'}\n",
      "\n",
      "Example 55:\n",
      "{'en': 'Installation', 'nl': 'Installatie'}\n",
      "\n",
      "Example 56:\n",
      "{'en': 'You should install the kdebase package which contains & konqueror; before attempting to compile this package.', 'nl': 'U dient eerst het pakket kdebase, dat & konqueror; bevat, te installeren voordat u dit pakket probeert te compileren.'}\n",
      "\n",
      "Example 57:\n",
      "{'en': 'Gardner Bell', 'nl': 'Gardner Bell'}\n",
      "\n",
      "Example 58:\n",
      "{'en': 'gbell72@rogers. com', 'nl': 'gbell72@rogers.com'}\n",
      "\n",
      "Example 59:\n",
      "{'en': 'ROLES_OF_TRANSLATORS', 'nl': '& Otto.Bruggeman; Bram.Schoenmakers; Tom.Albers; Natalie.Koning;'}\n",
      "\n",
      "Example 60:\n",
      "{'en': '2004-07-21 3.10.00', 'nl': '2004-07-21 3.10.00'}\n",
      "\n",
      "Example 61:\n",
      "{'en': 'The crashes plugin bookmarks a list of websites that & konqueror; has crashed on.', 'nl': 'De Vastloper-plugin maakt een lijst van websites die er voor gezorgd hebben dat & konqueror; vastliep.'}\n",
      "\n",
      "Example 62:\n",
      "{'en': 'KDE', 'nl': 'KDE'}\n",
      "\n",
      "Example 63:\n",
      "{'en': 'kdeaddons', 'nl': 'kdeaddons'}\n",
      "\n",
      "Example 64:\n",
      "{'en': 'konqueror', 'nl': 'konqueror'}\n",
      "\n",
      "Example 65:\n",
      "{'en': 'plugins', 'nl': 'plugins'}\n",
      "\n",
      "Example 66:\n",
      "{'en': 'The Crashes Plugin', 'nl': 'De Vastloper-plugin'}\n",
      "\n",
      "Example 67:\n",
      "{'en': 'Using the Crashes Plugin', 'nl': 'Het gebruik van de Vastloper-plugin'}\n",
      "\n",
      "Example 68:\n",
      "{'en': 'To use the crashes plugin point & konqueror; to Tools Crashes. Here you can select to view the site that & konqueror; has crashed on or clear the list of crashes, if any.', 'nl': 'Om de Vastloper-plugin te gebruiken selecteert u in & konqueror; Hulpmiddelen Vastlopers. Hier kunt u zien op welke sites & konqueror; is vastgelopen of de lijst wissen.'}\n",
      "\n",
      "Example 69:\n",
      "{'en': 'Credits', 'nl': 'Dankbetuigingen'}\n",
      "\n",
      "Example 70:\n",
      "{'en': 'The crashesplugin is Copyright & copy; 2002-2003 Alexander Kellett lypanov@kde. org.', 'nl': 'De Vastloper-plugin is Copyright & copy; 2002-2003 Alexander Kellett lypanov@kde.org.'}\n",
      "\n",
      "Example 71:\n",
      "{'en': 'Gardner Bell', 'nl': 'Gardner Bell'}\n",
      "\n",
      "Example 72:\n",
      "{'en': 'gbell72@rogers. com', 'nl': 'gbell72@rogers.com'}\n",
      "\n",
      "Example 73:\n",
      "{'en': 'ROLES_OF_TRANSLATORS', 'nl': '& Niels.Reedijk; Bram.Schoenmakers; Tom.Albers; Natalie.Koning;'}\n",
      "\n",
      "Example 74:\n",
      "{'en': '2004-09-15 3.10.00', 'nl': '2004-09-15 3.10.00'}\n",
      "\n",
      "Example 75:\n",
      "{'en': 'The directory view filter allows you to choose which items are displayed in a directory.', 'nl': 'De mapweergavefilter maakt het mogelijk aan te geven welke items getoond moeten worden in een map.'}\n",
      "\n",
      "Example 76:\n",
      "{'en': 'KDE', 'nl': 'KDE'}\n",
      "\n",
      "Example 77:\n",
      "{'en': 'kdeaddons', 'nl': 'kdeaddons'}\n",
      "\n",
      "Example 78:\n",
      "{'en': 'konqueror', 'nl': 'konqueror'}\n",
      "\n",
      "Example 79:\n",
      "{'en': 'plugins', 'nl': 'plugins'}\n",
      "\n",
      "Example 80:\n",
      "{'en': 'dirfilter', 'nl': 'dirfilter'}\n",
      "\n",
      "Example 81:\n",
      "{'en': 'The View Filter', 'nl': 'Het weergavefilter'}\n",
      "\n",
      "Example 82:\n",
      "{'en': 'Using the View Filter', 'nl': 'Het gebruik van het weergavefilter'}\n",
      "\n",
      "Example 83:\n",
      "{'en': \"The View Filter plugin (also known as dirfilter) can be accessed in the & konqueror; menubar under Tools View Filter. This plugin allows you to filter & konqueror; 's current working directory in a variety of ways.\", 'nl': 'Het weergavefilter (ook bekend als dirfilter -plugin) kan worden opgeroepen door in het & konqueror; menu te kiezen voor Hulpmiddelen Weergavefilter. De plugin maakt het mogelijk om de inhoud van een map te filteren.'}\n",
      "\n",
      "Example 84:\n",
      "{'en': 'Credits', 'nl': 'Dankbetuigingen'}\n",
      "\n",
      "Example 85:\n",
      "{'en': 'The dirfilter plugin is copyright & copy; 2000-2002 Dawit Alemayehu adawit@kde. org.', 'nl': 'De dirfilter-plugin is copyright & copy; 2000-2002 Dawit Alemayehu adawit@kde.org.'}\n",
      "\n",
      "Example 86:\n",
      "{'en': 'Gardner Bell', 'nl': 'Gardner Bell'}\n",
      "\n",
      "Example 87:\n",
      "{'en': 'gbell72@rogers. com', 'nl': 'gbell72@rogers.com'}\n",
      "\n",
      "Example 88:\n",
      "{'en': 'ROLES_OF_TRANSLATORS', 'nl': '& Niels.Reedijk; Bram.Schoenmakers; Tom.Albers; Natalie.Koning;'}\n",
      "\n",
      "Example 89:\n",
      "{'en': '2006-02-26 3.5.1', 'nl': '2006-02-26 3.5.1'}\n",
      "\n",
      "Example 90:\n",
      "{'en': 'The DOM Tree Viewer allows a developer to view the styles, attributes and elements of a web document.', 'nl': 'De DOM -boomstructuurweergave maakt het mogelijk voor een ontwikkelaar om de stijlen, attributen en elementen van een webpagina te bekijken.'}\n",
      "\n",
      "Example 91:\n",
      "{'en': 'KDE', 'nl': 'KDE'}\n",
      "\n",
      "Example 92:\n",
      "{'en': 'kdeaddons', 'nl': 'kdeaddons'}\n",
      "\n",
      "Example 93:\n",
      "{'en': 'konqueror', 'nl': 'konqueror'}\n",
      "\n",
      "Example 94:\n",
      "{'en': 'plugins', 'nl': 'plugins'}\n",
      "\n",
      "Example 95:\n",
      "{'en': 'DOMTreeViewer', 'nl': 'DOMTreeViewer'}\n",
      "\n",
      "Example 96:\n",
      "{'en': 'The DOM Tree Viewer', 'nl': 'De DOM -boomstructuurweergave'}\n",
      "\n",
      "Example 97:\n",
      "{'en': 'Using the DOM Tree Viewer', 'nl': 'Het gebruik van de DOM -boomstructuurweergave'}\n",
      "\n",
      "Example 98:\n",
      "{'en': 'To begin using The DOM Tree Viewer in & konqueror;, select Tools Show DOM Tree. This will open a new window which displays the DOM of the current web page you are viewing. By default the display starts at the root node of the document, example: documentElement, all other nodes that you will see are children or descendants of the root node.', 'nl': \"Om de DOM -boomstructuurweergave in & konqueror; te gebruiken kiest u het menu Hulpmiddelen DOM-boomstructuur tonen. Dit opent een nieuw venster dat de DOM van de huidige webpagina toont. Standaard start de weergave in het hoofditem van het document, bijvoorbeeld documentElement; alle andere items zullen daaronder 'hangen'.\"}\n",
      "\n",
      "Example 99:\n",
      "{'en': 'If you wish to manipulate the DOM tree in any way you will have to use an external script to do so.', 'nl': 'Om de DOM-structuur te wijzigen hebt u andere applicaties nodig.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "eng_nl = load_dataset(\"kde4\", lang1=\"en\", lang2=\"nl\", download_mode=\"force_redownload\")\n",
    "\n",
    "# Checking the quality of translation\n",
    "for i in range(100):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(eng_nl[\"train\"][i][\"translation\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c95a3e-a0a9-4957-b6fd-f3cb2330ff4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 08:10:21.950971: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1743667821.951286 1150602 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46672 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "All model checkpoint layers were used when initializing TFMBartForConditionalGeneration.\n",
      "\n",
      "All the layers of TFMBartForConditionalGeneration were initialized from the model checkpoint at facebook/mbart-large-cc25.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMBartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a21d0a17284c6d8df74566851ea4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/172854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79e1ff52da34e428b0458a1c2cedc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256027648 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 99s 380ms/step - loss: 4.3371 - accuracy: 0.7007 - val_loss: 1.0379 - val_accuracy: 0.9358\n",
      "Epoch 2/13\n",
      "200/200 [==============================] - 72s 363ms/step - loss: 0.8228 - accuracy: 0.9316 - val_loss: 0.4424 - val_accuracy: 0.9433\n",
      "Epoch 3/13\n",
      "200/200 [==============================] - 73s 365ms/step - loss: 0.4663 - accuracy: 0.9432 - val_loss: 0.2961 - val_accuracy: 0.9538\n",
      "Epoch 4/13\n",
      "200/200 [==============================] - 73s 364ms/step - loss: 0.3381 - accuracy: 0.9483 - val_loss: 0.2679 - val_accuracy: 0.9570\n",
      "Epoch 5/13\n",
      "200/200 [==============================] - 73s 366ms/step - loss: 0.3102 - accuracy: 0.9519 - val_loss: 0.2449 - val_accuracy: 0.9610\n",
      "Epoch 6/13\n",
      "200/200 [==============================] - 73s 366ms/step - loss: 0.3087 - accuracy: 0.9528 - val_loss: 0.2483 - val_accuracy: 0.9583\n",
      "Epoch 7/13\n",
      "200/200 [==============================] - 73s 364ms/step - loss: 0.2912 - accuracy: 0.9544 - val_loss: 0.2263 - val_accuracy: 0.9615\n",
      "Epoch 8/13\n",
      "200/200 [==============================] - 72s 363ms/step - loss: 0.2808 - accuracy: 0.9534 - val_loss: 0.2403 - val_accuracy: 0.9577\n",
      "Epoch 9/13\n",
      "200/200 [==============================] - 73s 365ms/step - loss: 0.2621 - accuracy: 0.9566 - val_loss: 0.2248 - val_accuracy: 0.9602\n",
      "Epoch 10/13\n",
      "200/200 [==============================] - 73s 364ms/step - loss: 0.2703 - accuracy: 0.9534 - val_loss: 0.2084 - val_accuracy: 0.9629\n",
      "Epoch 11/13\n",
      "200/200 [==============================] - 73s 363ms/step - loss: 0.2493 - accuracy: 0.9566 - val_loss: 0.2073 - val_accuracy: 0.9622\n",
      "Epoch 12/13\n",
      "200/200 [==============================] - 73s 363ms/step - loss: 0.2372 - accuracy: 0.9580 - val_loss: 0.1998 - val_accuracy: 0.9634\n",
      "Epoch 13/13\n",
      "200/200 [==============================] - 73s 364ms/step - loss: 0.2334 - accuracy: 0.9582 - val_loss: 0.1905 - val_accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:397: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 1024, 'num_beams': 5}\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Split the dataset for fine tuning\n",
    "split_datasets = eng_nl[\"train\"].train_test_split(test_size=0.1) # 90% train, 10% test\n",
    "\n",
    "# 2. Initialize TensorFlow model\n",
    "model = TFMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\n",
    "    \"facebook/mbart-large-50\",\n",
    "    src_lang=\"en_XX\",\n",
    "    tgt_lang=\"nl_XX\"\n",
    ")\n",
    "\n",
    "# 3. Preprocessing function\n",
    "def preprocess(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"nl\"] for ex in examples[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": model_inputs[\"input_ids\"],\n",
    "        \"attention_mask\": model_inputs[\"attention_mask\"],\n",
    "        \"labels\": labels[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# 4. Prepare TF Dataset\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
    "\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names\n",
    ")\n",
    "\n",
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "tf_val_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# 5. Configure Training\n",
    "num_train_steps = len(tf_train_dataset) * 10\n",
    "\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.05,\n",
    "    num_warmup_steps=0,  # 10% warmup\n",
    ")\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# 6. Train the model\n",
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_val_dataset,\n",
    "    epochs=13,\n",
    "    steps_per_epoch=200,\n",
    "    validation_steps=200\n",
    ")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"model_en_nl\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"model_en_nl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ac2d843-50f2-4c8c-bf10-fb4a59feebcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMBartForConditionalGeneration.\n",
      "\n",
      "All the layers of TFMBartForConditionalGeneration were initialized from the model checkpoint at facebook/mbart-large-cc25.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMBartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88085f5ef24d45498e2c83bb8dd7e081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/172854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6e1b438bbc41b6a829dc7778abbeae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256027648 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 94s 381ms/step - loss: 40.2269 - accuracy: 0.0126 - val_loss: 17.8227 - val_accuracy: 0.0209\n",
      "Epoch 2/13\n",
      "200/200 [==============================] - 72s 362ms/step - loss: 18.0805 - accuracy: 0.0137 - val_loss: 14.0046 - val_accuracy: 0.0207\n",
      "Epoch 3/13\n",
      "200/200 [==============================] - 72s 363ms/step - loss: 14.0246 - accuracy: 0.0143 - val_loss: 11.9279 - val_accuracy: 0.0208\n",
      "Epoch 4/13\n",
      "200/200 [==============================] - 73s 364ms/step - loss: 12.0697 - accuracy: 0.0126 - val_loss: 10.5704 - val_accuracy: 0.0261\n",
      "Epoch 5/13\n",
      "200/200 [==============================] - 73s 363ms/step - loss: 10.5271 - accuracy: 0.0146 - val_loss: 9.0743 - val_accuracy: 0.0243\n",
      "Epoch 6/13\n",
      "200/200 [==============================] - 72s 362ms/step - loss: 8.9076 - accuracy: 0.0738 - val_loss: 7.2989 - val_accuracy: 0.8829\n",
      "Epoch 7/13\n",
      "200/200 [==============================] - 72s 361ms/step - loss: 7.0614 - accuracy: 0.6702 - val_loss: 5.3268 - val_accuracy: 0.9269\n",
      "Epoch 8/13\n",
      "200/200 [==============================] - 73s 364ms/step - loss: 5.0139 - accuracy: 0.8431 - val_loss: 3.2976 - val_accuracy: 0.9306\n",
      "Epoch 9/13\n",
      "200/200 [==============================] - 73s 367ms/step - loss: 3.0309 - accuracy: 0.9113 - val_loss: 1.5607 - val_accuracy: 0.9415\n",
      "Epoch 10/13\n",
      "200/200 [==============================] - 74s 369ms/step - loss: 1.6774 - accuracy: 0.9265 - val_loss: 0.7948 - val_accuracy: 0.9468\n",
      "Epoch 11/13\n",
      "200/200 [==============================] - 72s 362ms/step - loss: 0.9621 - accuracy: 0.9332 - val_loss: 0.5947 - val_accuracy: 0.9460\n",
      "Epoch 12/13\n",
      "200/200 [==============================] - 73s 364ms/step - loss: 0.7275 - accuracy: 0.9371 - val_loss: 0.5326 - val_accuracy: 0.9452\n",
      "Epoch 13/13\n",
      "200/200 [==============================] - 73s 366ms/step - loss: 0.6285 - accuracy: 0.9386 - val_loss: 0.4626 - val_accuracy: 0.9494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:397: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 1024, 'num_beams': 5}\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Split the dataset for fine tuning\n",
    "split_datasets = eng_nl[\"train\"].train_test_split(test_size=0.1) # 90% train, 10% test\n",
    "\n",
    "# 2. Initialize TensorFlow model\n",
    "model_nl_en = TFMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "tokenizer_nl_en = MBart50Tokenizer.from_pretrained(\n",
    "    \"facebook/mbart-large-50\",\n",
    "    src_lang=\"nl_XX\",  # Dutch is now the source language\n",
    "    tgt_lang=\"en_XX\"   # English is now the target language\n",
    ")\n",
    "\n",
    "# 3. Preprocessing function\n",
    "def preprocess_nl_en(examples):\n",
    "    # Swap source and target languages\n",
    "    inputs = [ex[\"nl\"] for ex in examples[\"translation\"]]  # Now using Dutch as input\n",
    "    targets = [ex[\"en\"] for ex in examples[\"translation\"]]  # Now using English as target\n",
    "    \n",
    "    model_inputs = tokenizer_nl_en(\n",
    "        inputs,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    with tokenizer_nl_en.as_target_tokenizer():\n",
    "        labels = tokenizer_nl_en(\n",
    "            targets,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": model_inputs[\"input_ids\"],\n",
    "        \"attention_mask\": model_inputs[\"attention_mask\"],\n",
    "        \"labels\": labels[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# 4. Prepare TF Dataset\n",
    "data_collator_nl_en = DataCollatorForSeq2Seq(tokenizer_nl_en, model=model_nl_en, return_tensors=\"tf\")\n",
    "tokenized_datasets_nl_en = split_datasets.map(\n",
    "    preprocess_nl_en,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names\n",
    ")\n",
    "\n",
    "tf_train_dataset_nl_en = model_nl_en.prepare_tf_dataset(\n",
    "    tokenized_datasets_nl_en[\"train\"],\n",
    "    collate_fn=data_collator_nl_en,\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "tf_val_dataset_nl_en = model_nl_en.prepare_tf_dataset(\n",
    "    tokenized_datasets_nl_en[\"test\"],\n",
    "    collate_fn=data_collator_nl_en,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# 5. Configure Training\n",
    "num_train_steps = len(tf_train_dataset_nl_en) * 10\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.05,\n",
    "    num_warmup_steps=int(num_train_steps * 0.1),  # 10% warmup\n",
    ")\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model_nl_en.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# 6. Train the model\n",
    "model_nl_en.fit(\n",
    "    tf_train_dataset_nl_en,\n",
    "    validation_data=tf_val_dataset_nl_en,\n",
    "    epochs=13,\n",
    "    steps_per_epoch=200,\n",
    "    validation_steps=200\n",
    ")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"model_nl_en\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"model_nl_en\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "y2c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
